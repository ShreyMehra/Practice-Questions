#SQL
select query_name, 
round(sum(rating / position)/count(*), 2) as quality,
round(sum(case when rating < 3 then 1 else 0 end) * 100 / count(*), 2) as poor_query_percentage
from Queries
group by query_name

#PANDAS
import pandas as pd

def queries_stats(queries: pd.DataFrame) -> pd.DataFrame:
    return queries.groupby('query_name').agg(
    quality=('rating', lambda x: round((x / queries.loc[x.index, 'position']).sum() / len(x), 2)),
    poor_query_percentage=('rating', lambda x: round((sum(x < 3) * 100) / len(x), 2))
).reset_index()
    
#PYSPARK
from pyspark.sql import functions as F
from pyspark.sql import SparkSession
from pyspark.sql.functions import round, sum, col

spark = SparkSession.builder.master("local").appName("Queries").getOrCreate()

data = [
    ("Dog", "Golden Retriever", 1, 5),
    ("Dog", "German Shepherd", 2, 5),
    ("Dog", "Mule", 200, 1),
    ("Cat", "Shirazi", 5, 2),
    ("Cat", "Siamese", 3, 3),
    ("Cat", "Sphynx", 7, 4)
]

queries = spark.createDataFrame(data, ["query_name", "result", "position", "rating"])

processed_data = queries.groupBy("query_name").agg(
    F.round((F.sum(queries["rating"] / queries["position"]) / F.count(queries["rating"])), 2).alias("quality"),
    F.round((F.sum(F.when(queries["rating"] <= 3, 1).otherwise(0)) * 100) / F.count(queries["rating"]), 2).alias("poor_query_percentage")
)

processed_data.show()
