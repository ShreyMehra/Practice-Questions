#SQL
select P.project_id, round(avg(E.experience_years), 2) as average_years
from Employee E join Project P on E.employee_id=P.employee_id
group by P.project_id

#PANDAS
import pandas as pd

def project_employees_i(project: pd.DataFrame, employee: pd.DataFrame) -> pd.DataFrame:
    return employee.merge(project, on='employee_id').groupby('project_id')['experience_years'].mean().round(2).reset_index(name='average_years')

#PYSPARK
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

spark = SparkSession.builder.appName("Dataset").getOrCreate()

project_data = [
    (1, 1), (1, 2), (1, 3), (2, 1), (2, 4)
]
project_schema = StructType([
    StructField("project_id", IntegerType(), True),
    StructField("employee_id", IntegerType(), True)
])
Project = spark.createDataFrame(project_data, schema=project_schema)

employee_data = [
    (1, "Khaled", 3), (2, "Ali", 2), (3, "John", 1), (4, "Doe", 2)
]
employee_schema = StructType([
    StructField("employee_id", IntegerType(), True),
    StructField("name", StringType(), True),
    StructField("experience_years", IntegerType(), True)
])
Employee = spark.createDataFrame(employee_data, schema=employee_schema)

processed_data = Employee.join(Project, on='employee_id').groupby('project_id').mean('experience_years').withColumn('average_years', round(col('avg(experience_years)'), 2)).select('project_id', 'average_years')

processed_data.show()
