#SQL
select A1.machine_id, round(avg(A2.timestamp-A1.timestamp), 3) as processing_time 
from Activity A2 join Activity A1 on A2.machine_id=A1.machine_id and A2.process_id=A1.process_id and A1.activity_type="start" and A2.activity_type <> A1.activity_type
group by A1.machine_id

#PANDAS
import pandas as pd

def get_average_time(activity: pd.DataFrame) -> pd.DataFrame:
    activity2 = activity
    processed_data = activity.merge(activity2, on=['machine_id', 'process_id'])
    processed_data = processed_data[(processed_data['activity_type_x']=='start') & (processed_data['activity_type_y']=='end')]
    processed_data['diff'] = processed_data['timestamp_y']-processed_data['timestamp_x']
    processed_data = processed_data.groupby('machine_id')['diff'].mean().round(3).reset_index(name='processing_time')
    return processed_data

#PYSPARK
from pyspark.sql import SparkSession
from pyspark.sql.functions import round, col, avg

spark = SparkSession.builder.master("local").appName("Create Activity DF").getOrCreate()

data = [
    (0, 0, "start", 0.712),
    (0, 0, "end", 1.520),
    (0, 1, "start", 3.140),
    (0, 1, "end", 4.120),
    (1, 0, "start", 0.550),
    (1, 0, "end", 1.550),
    (1, 1, "start", 0.430),
    (1, 1, "end", 1.420),
    (2, 0, "start", 4.100),
    (2, 0, "end", 4.512),
    (2, 1, "start", 2.500),
    (2, 1, "end", 5.000)
]

columns = ["machine_id", "process_id", "activity_type", "timestamp"]

activity = spark.createDataFrame(data, columns)
activity2 = spark.createDataFrame(data, columns)

processed_data = activity.alias("A1").join(activity.alias("A2"), ((col("A2.machine_id") == col("A1.machine_id")) & (col("A2.process_id") == col("A1.process_id")) & (col("A1.timestamp") > col("A2.timestamp")))).groupBy(col("A1.machine_id")).agg(round(avg(col("A1.timestamp") - col("A2.timestamp")), 3).alias("processing_time"))
processed_data.show()