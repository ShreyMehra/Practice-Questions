#SQL
select name, population, area
from World
where area >= 3000000 or population >= 25000000

#PANDAS
import pandas as pd

def big_countries(world: pd.DataFrame) -> pd.DataFrame:
    return world[(world['area'] >= 3000000 ) | (world['population'] >= 25000000)][['name', 'population', 'area']]

#PYSPARK
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType
from pyspark.sql.functions import col

spark = SparkSession.builder.appName("WorldTable").getOrCreate()

data = [
    ("Afghanistan", "Asia", 652230, 25500100, 20343000000),
    ("Albania", "Europe", 28748, 2831741, 12960000000),
    ("Algeria", "Africa", 2381741, 37100000, 188681000000),
    ("Andorra", "Europe", 468, 78115, 3712000000),
    ("Angola", "Africa", 1246700, 20609294, 100990000000)
]

schema = StructType([
    StructField("name", StringType(), True),
    StructField("continent", StringType(), True),
    StructField("area", LongType(), True),
    StructField("population", LongType(), True),
    StructField("gdp", LongType(), True)
])

world = spark.createDataFrame(data, schema)

processed_data = world.where((col('area') >= 3000000 ) | (col('population') >= 25000000))
processed_data.select('name', 'population', 'area').show()